\chapter{关键技术分析和解决}
\label{cha:keytech}

实现本系统可采用的技术很多，不同的技术有着不同的特性，常常解决不同的问题，也有着各种不同的缺陷。
本章将根据本系统的特点、性能，及各种技术本身的特性，进行权衡取舍，合理地实现各种功能。

同时，由于系统未曾实现，所以对于关键技术的分析和解决都是思想性的，常常会基于一些假设。
当然，这种分析和解决在实际中并不一定成立，但能提供一个重要的参考和指导。并且，在实际开发和测试中，
这些技术将被逐渐使用。

本系统使用的关键技术主要针对于两类功能：个性化信息建模和隐私保护（\ref{sec:keyPersonal}），页面排序技术（\ref{sec:keyPageRank}）。

\section{个性化信息处理技术}
\label{sec:keyPersonal}

本节的技术主要基于如下假设：

\begin{compactitem}
\item
不同的兴趣在更高抽象层中可能是一样的\cite{Kimpl03}。
\item
越概括的兴趣往往是越长久的兴趣，越具体的兴趣往往是越短期的兴趣。
\item
根据用户行为的上下文在准确定位用户兴趣方面很重要，常可以消除自然语言的歧义。
关于这一点，这里指的是当把一系列网页分类，实现概括兴趣和具体兴趣的层次结构。
对于具体兴趣，当它位于概括兴趣的上下文之下时，其语义常常就不会有歧义。
比如Apple在Computer的上下文之后，就会被当作苹果电脑，而不是苹果这种水果。
\end{compactitem}

同时，应该注意的是，这些假设在一定条件是合乎情理的，但并非总是如此。

实现个性化信息处理技术主要包括个性化信息抽取、建模和隐私保护技术。

\subsection{个性化信息抽取}
\label{sub:keyExtract}

对于不同的数据来源往往使用不同的抽取技术。

对于用户查询的关键词，基本不用做太多处理。因为它们很简短，并且常常本身就是关键词。

对于用户历史记录和收藏，系统先提取网页摘要信息，因为它们中常常包含非结构化的数据，
并且直接对网页文本进行操作，常常因为文本模型的维度过高而性能低下。提取网页的技术一般包括\cite{Shen04}：
\begin{compactitem}
\item
Luhn's Summarization Method：是一种基于关键句子提取的方法。
它对每个句子定义一个重要系数，重要系数高的句子被用来形成摘要。
重要系数的计算分两步：先取词频在上下阈值之间的词建立重要词库；再根据词库计算每个句子的重要系数。
\item
Latent Semantic
Analysis：其特点在于它用非常高维的“语义空间”中的点来表示词条和相关内容。
\item
Content Body Identification by Page Layout
Analysis：根据余弦相似度，找到核心功能（句子），提取摘要。
\item
Supervised
Summarization：这种方法先从网页中提取一系列特征，再应用一种有监督的机器学习算法，
训练摘要提取器是否把某一句子加入到摘要中的鉴别能力。再利用摘要提取器进行提取。
\end{compactitem}

这些方法各有特点，而论文\cite{Shen04}把四种方法综合在一起提高摘要质量。而本系统也将使用这种方法。

而对于用户对于搜索结果的点击选择，由于搜索结果本身是摘要信息，所以不需重新提取。

这样最原始的个性化信息就包括一些关键词和一些摘要信息。当然可以在些过程对这些个性化信息进行一定的清洁处理。

\subsection{建模和隐私保护技术} % 文本分类
\label{sub:keyTree}

建模和隐私保护技术主要包括三个技术：模型构建、隐私量化和关键词过滤。

\begin{description}
\item[模型构建]

论文\cite{YaboYu07}描述了一种构建个性化信息层次化树状模型的方法。该方法基于这样一个假设：频繁出现的条目是用户感兴趣的话题。
实际上，这种假设在很多系统实现中都存在，并且具有较高的合理性。

系统将借鉴这种方法来实现。不过条目提取的数据为\ref{sub:keyExtract}的结果：现成的关键词条目和摘要信息。
这一步不采用原始网始数据的原因是网页异构化程度太高，并且直接使用网页效率无法保证。

对于关键词条目的提取，主要采用了分词技术和文本分类的一些相关技术\cite{GaoJie04}。

关键词条目提取之后，系统设置一个阈值从这些条目中选中频繁条目，这些频繁条目将被用来构造一个基于条目的个性化层次结构树状模型。
模型至项向下生成，主要关注各个条目之间的联系，而不是简单把所有条目分组。条目之间的关系概括为相似条目、父子条目和不相关条目。
关系的生成可以简单地通过统计词频和词在不同文档中的分布情况实现，方法与TF*IDF模型的思想类似。
根据这些关系，树状模型可以很容易的被构造出来。

在构造树状模型的同时计算每个结点的支持度（Support），这主要用于量化隐私。

\item[隐私量化]

通过引进信息论中Self-Information或Surprisal的概念来衡量隐私程序\cite{YaboYu07}。
根据概率论和信息论的结果，并且基于越具体敏感的条目，其Self-Information越大的假设，就可以使用每个条目（结点）的支持度来衡量隐私程度。
这样，可以引进一个参数，表示用户希望公开其隐私程度的阈值。这个阈值与支持度存在一个函数关系，这样阈值的提供，
可以灵活把一些隐私程序不高的树状部分提取出来。这实现也相当于一个截枝的过程，把过于敏感的关键词截掉。

同时，用户也可以较为明确地知道个性化隐私暴露的状况。

\item[关键词过滤]

需要进行关键词过滤的原因在于，虽然经过了隐私量化对隐私程度进行了设置，但是这只是去除了一些因为具体而敏感的条目。
像另外一些非常抽象的条目，或者某一方向上的兴趣，用户可能仍然不想让人知道，最常见的比如Sex，Women之类。
而这种条目是无法通过抽象泛化来过滤的。而系统或用户可以设定一些类似的关键词，这样，这方面的隐私也得到了保护。

\ref{cha:tech}中已经提过系统将进行两次关键词过滤，并且两次过滤的关键词是不一样的。
这是因为有些词用户可能仅仅不想发给服务器，而只是想存储在客户端以获得更好的个性化搜索服务。
系统的第一次过滤，过滤掉用户不想存储在客户端的数据，这部分数据也不会用来构建层次化树状模型。
实际上，系统也可以用关键词过滤来排除一些干扰词。第二次过滤则过滤掉用户不想发给服务器端的词。

关键词过滤技术实际上是相当简单的，本身基本没有什么复杂的算法。可以直接遍历被过滤的数据，匹配关键词库并除去匹配的数据。

\end{description}

\section{页面排序技术}
\label{sec:keyPageRank}

系统进行了两次页面重排，分别在服务器和客户端进行。这两次重排所用的技术基本上是类似的，
只是页面数量和个性化信息不同。

由于Google服务器返回的结果是已经排好序的，所以两次重排都是在原有排序基础上加入个性化信息进行调整。
调整的算法很简单：

\begin{compactenum}
\item
取若干条搜索结果页面摘要，对这些摘要同样进行生成个性化信息层次化树状模型类似的操作，
生成页面的树状模型。
\item
把结果与个性化信息层次化树状模型进行比较。
比较的方法是应用余弦相似度公式比较两者的相似度，然后利用相似度生成一个初步的纯个性化排序。
\item
把纯个性化排序与输入的已排好序的页面进行线性加权合成，生成新的排序。
\end{compactenum}

这种排序方法很简单，效率很高，不过要取得更好的效果，排序时的加权比重、选取的页面数目等都要在实验中调整。
另外，也可以考虑一些更复杂的加权方式。
